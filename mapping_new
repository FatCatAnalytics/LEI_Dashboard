import asyncio
import aiohttp
import pandas as pd
import logging
from sentence_transformers import SentenceTransformer, util
import urllib.parse
import time

# Setup logging
logging.basicConfig(level=logging.INFO)

# Load the fine-tuned model
model_path = 'fine-tuned-model'
model = SentenceTransformer(model_path)

# Track the number of requests made and the time of the first request in the current window
REQUEST_LIMIT = 45
TIME_WINDOW = 60  # seconds
requests_made = 0
start_time = time.time()

# Asynchronous function to fetch data from the API with retry mechanism
async def fetch_company_data(session, name, retries=3, backoff_factor=2):
    global requests_made, start_time

    # Check if the limit has been reached, and if so, wait for the time window to reset
    if requests_made >= REQUEST_LIMIT:
        elapsed_time = time.time() - start_time
        if elapsed_time < TIME_WINDOW:
            sleep_time = TIME_WINDOW - elapsed_time
            logging.info(f"Rate limit reached. Pausing for {sleep_time} seconds...")
            await asyncio.sleep(sleep_time)
        # Reset the request count and start time
        requests_made = 0
        start_time = time.time()

    encoded_company_name = urllib.parse.quote(name)
    url = f"https://api.gleif.org/api/v1/lei-records?page[size]=50&page[number]=1&filter[entity.names]={encoded_company_name}"

    for attempt in range(retries):
        try:
            async with session.get(url) as response:
                logging.info(f"Fetching data for {name} - Status: {response.status}")
                if response.status == 429:
                    logging.warning(f"Rate limit exceeded for {name}. Pausing for 60 seconds...")
                    await asyncio.sleep(60)  # Pause for 60 seconds if rate limited
                    continue
                response.raise_for_status()
                data = await response.json()
                
                # Increment the request counter after a successful request
                requests_made += 1
                return name, data
        except aiohttp.ClientError as e:
            logging.error(f"Error fetching data for {name}: {e}")
            if attempt == retries - 1:
                return name, None
            await asyncio.sleep(backoff_factor * (2 ** attempt))

    return name, None

# Asynchronous function to process a list of company names
async def fetch_all_companies(names):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_company_data(session, name) for name in names]
        results = await asyncio.gather(*tasks)
        return results

# Function to get SentenceTransformer embeddings
def get_bert_embedding(text):
    return model.encode(text, convert_to_tensor=True)

# Function to calculate cosine similarity using SentenceTransformer
def cosine_similarity_torch(vec1, vec2):
    return util.pytorch_cos_sim(vec1, vec2).item()

# Function to process and flatten the fetched data into a DataFrame
def process_results(results):
    records = []
    for name, data in results:
        if data and 'data' in data:
            for entry in data['data']:
                attributes = entry.get('attributes', {})
                entity_name = attributes.get('entity', {}).get('legalName', {}).get('name', 'N/A')
                lei = attributes.get('lei', 'N/A')
                address = attributes.get('entity', {}).get('legalAddress', {}).get('addressLines', ['N/A'])
                city = attributes.get('entity', {}).get('legalAddress', {}).get('city', 'N/A')
                country = attributes.get('entity', {}).get('legalAddress', {}).get('country', 'N/A')

                # Calculate BERT similarity score between the query name and the matched entity name
                query_embedding = get_bert_embedding(name)
                entity_embedding = get_bert_embedding(entity_name)
                similarity = cosine_similarity_torch(query_embedding, entity_embedding)

                # Additional features for ranking
                exact_match = 1 if name.lower() == entity_name.lower() else 0
                length_difference = abs(len(name) - len(entity_name))

                # Only add records where the matched entity name starts with the query name
                if entity_name.lower().startswith(name.lower()):
                    records.append({
                        "Query Name": name,
                        "Matched Entity Name": entity_name,
                        "LEI": lei,
                        "Address": ', '.join(address),
                        "City": city,
                        "Country": country,
                        "Similarity Score": similarity,
                        "Exact Match": exact_match,
                        "Length Difference": length_difference
                    })
        else:
            logging.warning(f"No data found for {name}")

    df = pd.DataFrame(records)
    return df

# Main function to fetch and process company data
def map_main(names):
    loop = asyncio.get_event_loop()
    results = loop.run_until_complete(fetch_all_companies(names))
    df = process_results(results).sort_values(by=["Similarity Score", "Exact Match", "Length Difference"], ascending=[False, False, True])
    return df

# Example usage
"""
if __name__ == "__main__":
    company_names = ["Apple", "Google llc", "Microsoft"]
    result_df = main(company_names)

    # Sort the DataFrame by similarity score, exact match, and length difference
    result_df_sorted = result_df.sort_values(by=["Exact Match", "Similarity Score", "Length Difference"], ascending=[False, False, True])

    # Print the sorted results
    print(result_df_sorted)
"""